{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import re \n",
    "import codecs as cds \n",
    "import nltk as nk \n",
    "from nltk import bigrams \n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAnalyzer:\n",
    "    def __init__(self, param):\n",
    "        self.param = param\n",
    "        self.dictionary = {\n",
    "            'phrases': [],\n",
    "            'tokens': [],\n",
    "            'avg_phrases': [],\n",
    "            'avg_words': [],\n",
    "            'hapax': [],\n",
    "            'ratio': [],\n",
    "            'bigrams': [],\n",
    "            'prob_cond': [],\n",
    "            'LMI': []\n",
    "        }\n",
    "\n",
    "    def calculate_token(self):\n",
    "        try:\n",
    "            total_length = sum(len(nk.word_tokenize(p)) for p in self.param)\n",
    "            self.dictionary['tokens'].append(total_length)\n",
    "            return total_length\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Error: {e}')\n",
    "\n",
    "\n",
    "    def quantity_sentences(self):\n",
    "        try:\n",
    "            counter = sum(1 for p in self.param)\n",
    "            self.dictionary['phrases'].append(counter)\n",
    "            return counter\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Error: {e}')\n",
    "\n",
    "\n",
    "    def sentences_avg(self):\n",
    "        try:\n",
    "            total_length = sum(len(p) for p in self.param)\n",
    "            avg_length = total_length / len(self.param)\n",
    "            self.dictionary['avg_phrases'].append(avg_length)\n",
    "            return avg_length\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Error: {e}')\n",
    "\n",
    "    def token_collection(self):\n",
    "        try:\n",
    "            total_tokens = [token for p in self.param for token in nk.word_tokenize(p)]\n",
    "            return total_tokens\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Error: {e}')\n",
    "\n",
    "    def word_avg(self):\n",
    "        try:\n",
    "            tokens = self.token_collection()\n",
    "            total_length = sum(len(tok) for tok in tokens)\n",
    "            average_length = total_length / len(tokens)\n",
    "            self.dictionary['avg_words'].append(average_length)\n",
    "            return average_length\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Error: {e}')\n",
    "\n",
    "\n",
    "    def hapax_collection(self):\n",
    "        try:\n",
    "            tokens = self.token_collection()\n",
    "            word_hapax = [word for word in set(tokens) if tokens.count(word) == 1]\n",
    "            self.dictionary['hapax'].append(len(word_hapax))\n",
    "            return word_hapax\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Error: {e}')\n",
    "\n",
    "\n",
    "    def incremental_frequency(self):\n",
    "        try:\n",
    "            incremental = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "            hapax = self.hapax_collection()\n",
    "            for length in incremental:\n",
    "                incremental_frequency = len(hapax) / length\n",
    "                print(f'\\tWith a corpus of length {length}, the distribution of hapax is {incremental_frequency}\\r')\n",
    "            return '\\tDistribution increment complete.'\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Error: {e}')\n",
    "\n",
    "\n",
    "    def ratio_noun_verbs(self):\n",
    "        try:\n",
    "            tokens = self.token_collection()\n",
    "            part_of_speech = nk.pos_tag(tokens)\n",
    "            noun_count = sum(1 for _, tag in part_of_speech if re.search(r'(NN|NNP|NNPS|NNS)', tag))\n",
    "            verb_count = sum(1 for _, tag in part_of_speech if re.search(r'(VBD|VBG|VBN|VBP|VBZ)', tag))\n",
    "\n",
    "            if verb_count == 0:\n",
    "                ratio = 'N/A'\n",
    "            else:\n",
    "                ratio = float(noun_count) / verb_count\n",
    "\n",
    "            self.dictionary['ratio'].append(ratio)\n",
    "            return ratio\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Error: {e}')\n",
    "\n",
    "\n",
    "\n",
    "    def part_of_speech_collection(self):\n",
    "        try:\n",
    "            tokens = self.token_collection()\n",
    "            part_of_speech = nk.pos_tag(tokens)\n",
    "            calculation_frequency = nk.FreqDist(part_of_speech)\n",
    "            frequency_feedback = calculation_frequency.most_common(10)\n",
    "            self.dictionary['bigrams'].append(frequency_feedback)\n",
    "            return frequency_feedback\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Error: {e}')\n",
    "        \n",
    "\n",
    "    def conditional_bigrams(self):\n",
    "        try:\n",
    "            tokens = self.token_collection()\n",
    "            bigrams_tokens = list(bigrams(tokens))\n",
    "\n",
    "            bigram_freq = {}\n",
    "            first_element_freq = {}\n",
    "            for bigram in bigrams_tokens:\n",
    "                bigram_freq[bigram] = bigram_freq.get(bigram, 0) + 1\n",
    "                first_element_freq[bigram[0]] = first_element_freq.get(bigram[0], 0) + 1\n",
    "\n",
    "            list_feedback = []\n",
    "            for bigram in bigram_freq:\n",
    "                prob_cond = bigram_freq[bigram] / first_element_freq[bigram[0]]\n",
    "                list_feedback.append((bigram, prob_cond))\n",
    "\n",
    "            tidy_list = sorted(list_feedback, key=lambda a: -a[1], reverse=False)[:10]\n",
    "\n",
    "            self.dictionary['prob_cond'].append(tidy_list)\n",
    "            return tidy_list\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Error: {e}')\n",
    "\n",
    "\n",
    "\n",
    "    def lmi(self):\n",
    "        try:\n",
    "            tokens = self.token_collection()\n",
    "            bigrams_tokens = list(nk.bigrams(tokens))\n",
    "\n",
    "            calculation_frequency = nk.FreqDist(bigrams_tokens)\n",
    "            n = len(tokens)\n",
    "\n",
    "            list_feedback = []\n",
    "            for bigram in calculation_frequency:\n",
    "                bigram_frequency = calculation_frequency[bigram]\n",
    "                first_element_frequency = sum(1 for token in tokens if token == bigram[0])\n",
    "                second_element_frequency = sum(1 for token in tokens if token == bigram[1])\n",
    "\n",
    "                numerator = bigram_frequency * n\n",
    "                denominator = first_element_frequency * second_element_frequency\n",
    "\n",
    "                if denominator != 0:\n",
    "                    mutual_information = math.log2(numerator / denominator)\n",
    "                    local_mutual_information = bigram_frequency * mutual_information\n",
    "                    list_feedback.append((bigram, local_mutual_information))\n",
    "\n",
    "            tidy_list = sorted(list_feedback, key=lambda a: -a[1])[:10]\n",
    "            self.dictionary['LMI'].append(tidy_list)\n",
    "            return tidy_list\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Error: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(first_par, second_par, output_file):\n",
    "    # Open and read the input files\n",
    "    input_1 = cds.open(first_par, 'r', 'utf-8')\n",
    "    input_2 = cds.open(second_par, 'r', 'utf-8')\n",
    "    raw_1 = input_1.read()\n",
    "    raw_2 = input_2.read()\n",
    "\n",
    "    # Tokenize the text into sentences using NLTK's Punkt tokenizer\n",
    "    splitter = nk.data.load('tokenizers/punkt/english.pickle')\n",
    "    phrase_1 = splitter.tokenize(raw_1)\n",
    "    phrase_2 = splitter.tokenize(raw_2)\n",
    "\n",
    "    # Create instances of TextAnalyzer to analyze the text\n",
    "    analyzer_1 = TextAnalyzer(phrase_1)\n",
    "    analyzer_2 = TextAnalyzer(phrase_2)\n",
    "\n",
    "    # Write the output to the specified file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        # Analyze the first text\n",
    "        f.write('--'*40 + '\\n')\n",
    "        f.write(f'Text: {first_par}\\n')\n",
    "        f.write('--'*40 + '\\n')\n",
    "\n",
    "        # Calculate the total number of tokens in the text\n",
    "        analyzer_1.calculate_token()\n",
    "        f.write(f'--Number of Tokens equal to {analyzer_1.dictionary[\"tokens\"][0]}\\n')\n",
    "\n",
    "        # Calculate the total number of sentences in the text\n",
    "        analyzer_1.quantity_sentences()\n",
    "        f.write(f'--Number of sentences equal to {analyzer_1.dictionary[\"phrases\"][0]}\\n')\n",
    "        \n",
    "        # Calculate the average sentence length in the text\n",
    "        analyzer_1.sentences_avg()\n",
    "        f.write(f'--The average sentence length is {analyzer_1.dictionary[\"avg_phrases\"][0]}\\n') \n",
    "\n",
    "        # Calculate the average word length in the text\n",
    "        analyzer_1.word_avg()\n",
    "        f.write(f'--The average word length is {analyzer_1.dictionary[\"avg_words\"][0]}\\n') \n",
    "\n",
    "        # Calculate the number of hapax legomena in the text\n",
    "        analyzer_1.hapax_collection()\n",
    "        f.write(f'--The number of hapax legomena is {analyzer_1.dictionary[\"hapax\"][0]}\\n')\n",
    "\n",
    "        # Calculate the incremental frequency of tokens in the text\n",
    "        f.write(f'{analyzer_1.incremental_frequency()}\\n')\n",
    "\n",
    "        # Calculate the noun-to-verb ratio in the text\n",
    "        analyzer_1.ratio_noun_verbs()\n",
    "        f.write(f'--The noun-to-verb ratio is {analyzer_1.dictionary[\"ratio\"][0]}\\n')\n",
    "        \n",
    "        # Find and print the 10 most frequent bigrams in the text\n",
    "        analyzer_1.part_of_speech_collection()\n",
    "        f.write('--List of the 10 most bigrams:\\n')\n",
    "        for bigrams, frequency in analyzer_1.dictionary[\"bigrams\"][0]:\n",
    "            f.write(f'{bigrams} with frequency {frequency}\\n')\n",
    "\n",
    "        # Calculate and print 10 bigrams with Conditional Probability in the text\n",
    "        analyzer_1.conditional_bigrams()\n",
    "        f.write('--List of 10 bigrams with Conditional Probability:\\n')\n",
    "        for bigrams, prob_cond in analyzer_1.dictionary[\"prob_cond\"][0]:\n",
    "            f.write(f'{bigrams} with conditional probability {prob_cond}\\n')\n",
    "\n",
    "        # Calculate and print 10 bigrams with associative strength in terms of LMI in the text\n",
    "        analyzer_1.lmi()\n",
    "        f.write('--List of 10 bigrams with associative strength in terms of LMI:\\n')\n",
    "        for bigrams in analyzer_1.dictionary[\"LMI\"][0]:\n",
    "            f.write(f'{bigrams[0]} with associative strength {bigrams[1]}\\n')\n",
    "\n",
    "        f.write('\\n\\n\\n')\n",
    "        \n",
    "        # Analyze the second text\n",
    "        f.write('--'*40 + '\\n')\n",
    "        f.write(f'Text: {second_par}\\n')\n",
    "        f.write('--'*40 + '\\n')\n",
    "\n",
    "        # Calculate the total number of tokens in the text\n",
    "        analyzer_2.calculate_token()\n",
    "        f.write(f'--Number of Tokens equal to {analyzer_2.dictionary[\"tokens\"][0]}\\n')\n",
    "\n",
    "        # Calculate the total number of sentences in the text\n",
    "        analyzer_2.quantity_sentences()\n",
    "        f.write(f'--Number of sentences equal to {analyzer_2.dictionary[\"phrases\"][0]}\\n')\n",
    "        \n",
    "        # Calculate the average sentence length in the text\n",
    "        analyzer_2.sentences_avg()\n",
    "        f.write(f'--The average sentence length is {analyzer_2.dictionary[\"avg_phrases\"][0]}\\n') \n",
    "\n",
    "        # Calculate the average word length in the text\n",
    "        analyzer_2.word_avg()\n",
    "        f.write(f'--The average word length is {analyzer_2.dictionary[\"avg_words\"][0]}\\n') \n",
    "\n",
    "        # Calculate the number of hapax legomena in the text\n",
    "        analyzer_2.hapax_collection()\n",
    "        f.write(f'--The number of hapax legomena is {analyzer_2.dictionary[\"hapax\"][0]}\\n')\n",
    "\n",
    "        # Calculate the incremental frequency of tokens in the text\n",
    "        f.write(f'{analyzer_2.incremental_frequency()}\\n')\n",
    "\n",
    "        # Calculate the noun-to-verb ratio in the text\n",
    "        analyzer_2.ratio_noun_verbs()\n",
    "        f.write(f'--The noun-to-verb ratio is {analyzer_2.dictionary[\"ratio\"][0]}\\n')\n",
    "        \n",
    "        # Find and print the 10 most frequent bigrams in the text\n",
    "        analyzer_2.part_of_speech_collection()\n",
    "        f.write('--List of the 10 most bigrams:\\n')\n",
    "        for bigrams, frequency in analyzer_2.dictionary[\"bigrams\"][0]:\n",
    "            f.write(f'{bigrams} with frequency {frequency}\\n')\n",
    "\n",
    "        # Calculate and print 10 bigrams with Conditional Probability in the text\n",
    "        analyzer_2.conditional_bigrams()\n",
    "        f.write('--List of 10 bigrams with Conditional Probability:\\n')\n",
    "        for bigrams, prob_cond in analyzer_2.dictionary[\"prob_cond\"][0]:\n",
    "            f.write(f'{bigrams} with conditional probability {prob_cond}\\n')\n",
    "\n",
    "        # Calculate and print 10 bigrams with associative strength in terms of LMI in the text\n",
    "        analyzer_2.lmi()\n",
    "        f.write('--List of 10 bigrams with associative strength in terms of LMI:\\n')\n",
    "        for bigrams in analyzer_2.dictionary[\"LMI\"][0]:\n",
    "            f.write(f'{bigrams[0]} with associative strength {bigrams[1]}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    first_path = '../text/dracula-UTF8.txt'\n",
    "    second_path = '../text/hydeandjack-UTF8.txt'\n",
    "    output_file = '../output/output_program_1.out'\n",
    "    main(first_path, second_path, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
