{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import re \n",
    "import codecs as cds \n",
    "import nltk as nk \n",
    "from nltk import bigrams \n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAnalyzer:\n",
    "    def __init__(self, par):\n",
    "        self.par = par\n",
    "        self.dictionary = {\n",
    "            \"phrases\": [],\n",
    "            \"tokens\": [],\n",
    "            \"avg_phrases\": [],\n",
    "            \"avg_words\": [],\n",
    "            \"hapax\": [],\n",
    "            \"ratio\": [],\n",
    "            \"bigrams\": [],\n",
    "            \"prob_cond\": [],\n",
    "            \"LMI\": []\n",
    "        }\n",
    "\n",
    "    def calculate_token(self):\n",
    "        total_length = 0\n",
    "        total_tokens = [] \n",
    "        for el in self.par:  \n",
    "            tokenized = nk.word_tokenize(el) \n",
    "            total_tokens = total_tokens + tokenized \n",
    "        total_length = len(total_tokens)\n",
    "        self.dictionary[\"tokens\"].append(total_length)\n",
    "        return total_length\n",
    "        \n",
    "    def quantity_sentences(self):\n",
    "        counter = 0 \n",
    "        for el in self.par:\n",
    "            el = 1 \n",
    "            counter += el \n",
    "        self.dictionary[\"phrases\"].append(counter)\n",
    "        return counter\n",
    "\n",
    "    def sentences_avg(self):\n",
    "        counter = 0\n",
    "        for el in self.par: \n",
    "            counter += len(el) \n",
    "        counter = counter / len(self.par) \n",
    "        self.dictionary[\"avg_phrases\"].append(counter)\n",
    "        return counter\n",
    "\n",
    "    def token_collection(self):\n",
    "        total_tokens = [] \n",
    "        for el in self.par: \n",
    "            tokenized = nk.word_tokenize(el) \n",
    "            total_tokens = total_tokens + tokenized \n",
    "        return total_tokens\n",
    "\n",
    "    def word_avg(self):\n",
    "        total_length = 0 \n",
    "        char_length = 0 \n",
    "        tokens = self.token_collection()\n",
    "        for el in tokens: \n",
    "            char_length = float(char_length + len(el)) \n",
    "            total_length = total_length + 1 \n",
    "        average_length = float(char_length / total_length) \n",
    "        self.dictionary[\"avg_words\"].append(average_length)\n",
    "        return average_length \n",
    "\n",
    "    def hapax_collection(self):\n",
    "        frequency = 1 \n",
    "        tokens = self.token_collection()\n",
    "        word_hapax = []\n",
    "        vocabular = set(tokens)\n",
    "        for el in vocabular:\n",
    "            token_frequency = tokens.count(el)\n",
    "            if token_frequency == frequency: \n",
    "                word_hapax.append(el) \n",
    "        self.dictionary[\"hapax\"].append(len(word_hapax))\n",
    "        return word_hapax\n",
    "\n",
    "    def incremental_frequency(self):\n",
    "        incremental = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "        el = 0\n",
    "        hapax = self.hapax_collection()\n",
    "        while el < len(incremental):\n",
    "            incremental_frequency = len(hapax) * 1.0 / incremental[el] \n",
    "            print(\"\\tWith a corpus of length {}, the distribution of hapax is {}\\r\".format(incremental[el], incremental_frequency)) \n",
    "            el = el + 1 \n",
    "        return \"\\tDistribution increment complete.\"\n",
    "\n",
    "    def ratio_noun_verbs(self):\n",
    "        tokens = self.token_collection()\n",
    "        part_of_speech = nk.pos_tag(tokens)\n",
    "        noun_count = 0\n",
    "        verb_count = 0\n",
    "        for tag in part_of_speech:\n",
    "            if re.search(r'(NN|NNP|NNPS|NNS)', tag[1]):\n",
    "                noun_count = noun_count + 1\n",
    "            elif re.search(r'(VBD|VBG|VBN|VBP|VBZ)', tag[1]):\n",
    "                verb_count = verb_count + 1\n",
    "        ratio = float(noun_count) / verb_count\n",
    "        self.dictionary[\"ratio\"].append(ratio)\n",
    "        return ratio\n",
    "\n",
    "    def part_of_speech_collection(self):\n",
    "        tokens = self.token_collection()\n",
    "        part_of_speech = nk.pos_tag(tokens)\n",
    "        list_feedback = []\n",
    "        for el in part_of_speech:\n",
    "            list_feedback.append(el)\n",
    "        calculation_frequency = nk.FreqDist(list_feedback)\n",
    "        frequency_feedback = calculation_frequency.most_common(10)\n",
    "        self.dictionary[\"bigrams\"].append(frequency_feedback)\n",
    "        return frequency_feedback\n",
    "\n",
    "    def conditional_bigrams(self):\n",
    "        tokens = self.token_collection()\n",
    "        bigrams_tokens = list(bigrams(tokens))\n",
    "        iterable_bigrams = set(bigrams_tokens)\n",
    "        list_feedback = []\n",
    "        for el in iterable_bigrams:\n",
    "            bigram_frequency = bigrams_tokens.count(el)\n",
    "            first_element_frequency = tokens.count(el[0])\n",
    "            conditional_probability = float(bigram_frequency / first_element_frequency)\n",
    "            list_feedback.append((el, conditional_probability))\n",
    "        tidy_list = sorted(list_feedback, key=lambda a: -a[1], reverse=False)\n",
    "        self.dictionary[\"prob_cond\"].append(tidy_list[:10])\n",
    "        return tidy_list[:10]\n",
    "\n",
    "    def lmi(self):\n",
    "        tokens = self.token_collection()\n",
    "        bigrams_tokens = list(bigrams(tokens))\n",
    "        iterable_bigrams = set(bigrams_tokens)\n",
    "        n = len(tokens)\n",
    "        list_feedback = []\n",
    "        for el in iterable_bigrams:\n",
    "            bigram_frequency = bigrams_tokens.count(el)\n",
    "            first_element_frequency = tokens.count(el[0])\n",
    "            second_element_frequency = tokens.count(el[1])\n",
    "            numerator = float(bigram_frequency * n)\n",
    "            denominator = float(first_element_frequency * second_element_frequency)\n",
    "            mutual_information = math.log(numerator / denominator, 2.0)\n",
    "            local_mutual_information = bigram_frequency * mutual_information\n",
    "            list_feedback.append((el, local_mutual_information))\n",
    "        tidy_list = sorted(list_feedback, key=lambda a: -a[1], reverse=False)\n",
    "        self.dictionary[\"LMI\"].append(tidy_list[:10])\n",
    "        return tidy_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(first_par, second_par):\n",
    "    input_1 = cds.open(first_par, \"r\", \"utf-8\")\n",
    "    input_2 = cds.open(second_par, \"r\", \"utf-8\")\n",
    "    raw_1 = input_1.read()\n",
    "    raw_2 = input_2.read()\n",
    "    splitter = nk.data.load('tokenizers/punkt/english.pickle')\n",
    "    phrase_1 = splitter.tokenize(raw_1)\n",
    "    phrase_2 = splitter.tokenize(raw_2)\n",
    "\n",
    "    analyzer_1 = TextAnalyzer(phrase_1)\n",
    "    analyzer_2 = TextAnalyzer(phrase_2)\n",
    "\n",
    "    print(\"--\"*60)\n",
    "    print(\"Text: {}\".format(first_par))\n",
    "    print(\"--\"*60)\n",
    "    analyzer_1.calculate_token()\n",
    "    print(\"--Number of Tokens equal to {}\".format(analyzer_1.dictionary[\"tokens\"][0]))\n",
    "    analyzer_1.quantity_sentences()\n",
    "    print(\"--Number of sentences equal to {}\".format(analyzer_1.dictionary[\"phrases\"][0]))\n",
    "    analyzer_1.sentences_avg()\n",
    "    print(\"--The average sentence length is {}\".format(analyzer_1.dictionary[\"avg_phrases\"][0])) \n",
    "    analyzer_1.word_avg()\n",
    "    print(\"--The average word length is {}\".format(analyzer_1.dictionary[\"avg_words\"][0])) \n",
    "    analyzer_1.hapax_collection()\n",
    "    print(\"--The number of hapax legomena is {}\".format(analyzer_1.dictionary[\"hapax\"][0])) \n",
    "    print(\"{}\".format(analyzer_1.incremental_frequency()))\n",
    "    analyzer_1.ratio_noun_verbs()\n",
    "    print(\"--The noun-to-verb ratio is {}\".format(analyzer_1.dictionary[\"ratio\"][0]))\n",
    "    analyzer_1.part_of_speech_collection()\n",
    "    print(\"--List of the 10 most bigrams:\\n {}\".format(analyzer_1.dictionary[\"bigrams\"]))\n",
    "    analyzer_1.conditional_bigrams()\n",
    "    print(\"--List of 10 bigrams with Conditional Probability:\\n {}\".format(analyzer_1.dictionary[\"prob_cond\"]))\n",
    "    analyzer_1.lmi()\n",
    "    print(\"--List of 10 bigrams with associative strength in terms of LMI:\\n {}\".format(analyzer_1.dictionary[\"LMI\"]))\n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"--\"*60)\n",
    "    print(\"Text: {}\".format(second_par))\n",
    "    print(\"--\"*60)\n",
    "    analyzer_2.calculate_token()\n",
    "    print(\"--Number of Tokens equal to {}\".format(analyzer_2.dictionary[\"tokens\"][0]))\n",
    "    analyzer_2.quantity_sentences()\n",
    "    print(\"--Number of sentences equal to {}\".format(analyzer_2.dictionary[\"phrases\"][0]))\n",
    "    analyzer_2.sentences_avg()\n",
    "    print(\"--The average sentence length is {}\".format(analyzer_2.dictionary[\"avg_phrases\"][0])) \n",
    "    analyzer_2.word_avg()\n",
    "    print(\"--The average word length is {}\".format(analyzer_2.dictionary[\"avg_words\"][0])) \n",
    "    analyzer_2.hapax_collection()\n",
    "    print(\"--The number of hapax legomena is {}\".format(analyzer_2.dictionary[\"hapax\"][0])) \n",
    "    print(\"{}\".format(analyzer_2.incremental_frequency()))\n",
    "    analyzer_2.ratio_noun_verbs()\n",
    "    print(\"--The noun-to-verb ratio is {}\".format(analyzer_2.dictionary[\"ratio\"][0]))\n",
    "    analyzer_2.part_of_speech_collection()\n",
    "    print(\"--List of the 10 most bigrams:\\n {}\".format(analyzer_2.dictionary[\"bigrams\"]))\n",
    "    analyzer_2.conditional_bigrams()\n",
    "    print(\"--List of 10 bigrams with Conditional Probability:\\n {}\".format(analyzer_2.dictionary[\"prob_cond\"]))\n",
    "    analyzer_2.lmi()\n",
    "    print(\"--List of 10 bigrams with associative strength in terms of LMI:\\n {}\".format(analyzer_2.dictionary[\"LMI\"]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Text: ../text/dracula-UTF8.txt\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "--Number of Tokens equal to 189228\n",
      "--Number of sentences equal to 8428\n",
      "--The average sentence length is 99.87446606549597\n",
      "--The average word length is 3.571929101401484\n",
      "--The number of hapax legomena is 5164\n",
      "\tWith a corpus of length 1000, the distribution of hapax is 5.164\n",
      "\tWith a corpus of length 2000, the distribution of hapax is 2.582\n",
      "\tWith a corpus of length 3000, the distribution of hapax is 1.7213333333333334\n",
      "\tWith a corpus of length 4000, the distribution of hapax is 1.291\n",
      "\tWith a corpus of length 5000, the distribution of hapax is 1.0328\n",
      "\tWith a corpus of length 6000, the distribution of hapax is 0.8606666666666667\n",
      "\tWith a corpus of length 7000, the distribution of hapax is 0.7377142857142858\n",
      "\tWith a corpus of length 8000, the distribution of hapax is 0.6455\n",
      "\tWith a corpus of length 9000, the distribution of hapax is 0.5737777777777778\n",
      "\tWith a corpus of length 10000, the distribution of hapax is 0.5164\n",
      "\tDistribution increment complete.\n",
      "--The noun-to-verb ratio is 1.3793900789668863\n",
      "--List of the 10 most bigrams:\n",
      " [[((',', ','), 11214), (('the', 'DT'), 7282), (('.', '.'), 7183), (('and', 'CC'), 5715), (('I', 'PRP'), 4834), (('to', 'TO'), 4412), (('of', 'IN'), 3569), (('a', 'DT'), 2852), (('in', 'IN'), 2399), (('he', 'PRP'), 1988)]]\n",
      "--List of 10 bigrams with Conditional Probability:\n",
      " [[(('self-surrender', 'of'), 1.0), (('considering', 'the'), 1.0), (('impartiality', 'of'), 1.0), ((\"'20\", ';'), 1.0), (('bunk', '.'), 1.0), (('embarrassed', '.'), 1.0), (('thorny', 'paths'), 1.0), (('Listen', 'to'), 1.0), (('promises', ':'), 1.0), (('hatchways', 'were'), 1.0)]]\n",
      "--List of 10 bigrams with associative strength in terms of LMI:\n",
      " [[((',', 'and'), 8483.432760750295), (('*', '*'), 4037.585850474514), ((':', '--'), 2961.0457518980793), (('Van', 'Helsing'), 2844.4247539680687), (('.', 'I'), 2622.61635553677), (('--', \"''\"), 2567.675548428885), (('of', 'the'), 2284.202509160511), (('.', 'The'), 1933.1492638166114), (('.', 'He'), 1754.0133617786198), (('.', \"''\"), 1629.5893981617921)]]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Text: ../text/hydeandjack-UTF8.txt\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "--Number of Tokens equal to 30595\n",
      "--Number of sentences equal to 956\n",
      "--The average sentence length is 145.85983263598325\n",
      "--The average word length is 3.6786729857819904\n",
      "--The number of hapax legomena is 2519\n",
      "\tWith a corpus of length 1000, the distribution of hapax is 2.519\n",
      "\tWith a corpus of length 2000, the distribution of hapax is 1.2595\n",
      "\tWith a corpus of length 3000, the distribution of hapax is 0.8396666666666667\n",
      "\tWith a corpus of length 4000, the distribution of hapax is 0.62975\n",
      "\tWith a corpus of length 5000, the distribution of hapax is 0.5038\n",
      "\tWith a corpus of length 6000, the distribution of hapax is 0.41983333333333334\n",
      "\tWith a corpus of length 7000, the distribution of hapax is 0.3598571428571429\n",
      "\tWith a corpus of length 8000, the distribution of hapax is 0.314875\n",
      "\tWith a corpus of length 9000, the distribution of hapax is 0.2798888888888889\n",
      "\tWith a corpus of length 10000, the distribution of hapax is 0.2519\n",
      "\tDistribution increment complete.\n",
      "--The noun-to-verb ratio is 1.7259180415114423\n",
      "--List of the 10 most bigrams:\n",
      " [[((',', ','), 2046), (('the', 'DT'), 1499), (('of', 'IN'), 930), (('and', 'CC'), 904), (('.', '.'), 890), (('I', 'PRP'), 640), (('to', 'TO'), 634), (('a', 'DT'), 605), ((';', ':'), 527), (('was', 'VBD'), 466)]]\n",
      "--List of 10 bigrams with Conditional Probability:\n",
      " [[(('mottled', 'pallor'), 1.0), (('disloyalty', ','), 1.0), (('term', '.'), 1.0), (('hurry', 'of'), 1.0), (('acts', 'of'), 1.0), (('frank', ','), 1.0), (('generation', ','), 1.0), (('kindling', 'spark'), 1.0), (('midst', 'of'), 1.0), (('lied.', '”'), 1.0)]]\n",
      "--List of 10 bigrams with associative strength in terms of LMI:\n",
      " [[(('’', 's'), 995.9552535597893), (('.', '“'), 910.4669252497815), ((';', 'and'), 772.6600242889918), ((',', '”'), 531.4077246698389), (('”', 'said'), 513.6920988737005), (('Mr.', 'Utterson'), 502.2523427931724), ((',', 'and'), 456.90924383793185), (('?', '”'), 411.11020404062515), (('”', '“'), 392.2706433596871), (('in', 'the'), 366.8695820313626)]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    first_path = '../text/dracula-UTF8.txt'\n",
    "    second_path = '../text/hydeandjack-UTF8.txt'\n",
    "    main(first_path, second_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
