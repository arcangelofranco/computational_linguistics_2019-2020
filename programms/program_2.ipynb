{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import re \n",
    "import codecs as cds \n",
    "import nltk as nk \n",
    "import math \n",
    "from nltk import FreqDist, ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAnalyzer:\n",
    "    def __init__(self, param):\n",
    "        self.param = param\n",
    "        self.dictionary = {\n",
    "            'NNP': [],\n",
    "            'phrases_NNP': [],\n",
    "            'phrases_length': [],\n",
    "            'GPE': [],\n",
    "            'person': [],\n",
    "            'nouns': [],\n",
    "            'verbs': [],\n",
    "            'dates': [],\n",
    "            'markov_order': []\n",
    "        }\n",
    "\n",
    "\n",
    "    def token_collection(self):\n",
    "        try:\n",
    "            total_tokens = [token for p in self.param for token in nk.word_tokenize(p)]\n",
    "            return total_tokens\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Error: {e}')\n",
    "\n",
    "    def noun_search(self):\n",
    "        try:\n",
    "            noun_list = []\n",
    "            for p in self.param:\n",
    "                tokens = nk.word_tokenize(p)\n",
    "                pos = nk.pos_tag(tokens)\n",
    "                tree_pos = nk.ne_chunk(pos, binary=False)\n",
    "                \n",
    "                # Estrae solo le entità PERSON e le filtra per nomi validi\n",
    "                name_phrases = [' '.join(noun[0] for noun in node.leaves()) for node in tree_pos if isinstance(node, nk.Tree) and node.label() == 'PERSON']\n",
    "                name_phrases = [name for name in name_phrases if name[0].isalpha() and name[0].isupper()]\n",
    "                \n",
    "                \n",
    "                noun_list.extend(name_phrases)\n",
    "            \n",
    "            total_noun = FreqDist(noun_list).most_common(10)\n",
    "            self.dictionary['NNP'].append(total_noun)\n",
    "            return total_noun\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Error: {e}')\n",
    "\n",
    "        \n",
    "    def phrase_searcher(self):\n",
    "        try:\n",
    "            top_nouns = [noun[0] for noun in self.noun_search()[:10]]\n",
    "            noun_set = set(top_nouns)\n",
    "            phrase_list = [p for p in self.param if any(noun in p for noun in noun_set)]\n",
    "            \n",
    "            self.dictionary['phrases_NNP'].append(phrase_list)\n",
    "            return phrase_list\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Error: {e}')\n",
    "\n",
    "\n",
    "        \n",
    "    def noun_set(self):\n",
    "        try:\n",
    "            noun_list = self.noun_search()\n",
    "            only_noun = set([noun[0] for noun in noun_list])\n",
    "            return only_noun\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Error: {e}')\n",
    "        \n",
    "    def phrase_length(self):\n",
    "        try:\n",
    "            noun_list = self.noun_search()\n",
    "            phrase_list = self.phrase_searcher()\n",
    "\n",
    "            temp_dict = {}\n",
    "\n",
    "            for noun in noun_list:\n",
    "                list_of_phrases = [phrase for phrase in phrase_list if noun[0] in phrase]\n",
    "                list_of_phrases.sort(key=len)\n",
    "                if list_of_phrases:\n",
    "                    shortest = 'La frase più corta che contiene il nome ' + noun[0] + ' è: ' + list_of_phrases[0]\n",
    "                    longest = 'La frase più lunga che contiene il nome ' + noun[0] + ' è: ' + list_of_phrases[-1]\n",
    "                    temp_dict[noun[0]] = shortest, longest\n",
    "\n",
    "            self.dictionary['phrases_length'].append(temp_dict)\n",
    "            return temp_dict\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Error: {e}')\n",
    "        \n",
    "\n",
    "    def geo_entity_search(self):\n",
    "        try:\n",
    "            phrase_list = self.phrase_searcher()\n",
    "            noun_set = self.noun_set()\n",
    "\n",
    "            gpe_list = []\n",
    "            for phrase in phrase_list:\n",
    "                tokens = nk.word_tokenize(phrase)\n",
    "                pos = nk.pos_tag(tokens)\n",
    "                tree = nk.ne_chunk(pos)\n",
    "                gpe_phrases = [' '.join(leaf[0] for leaf in node.leaves()) for node in tree if isinstance(node, nk.Tree) and node.label() == 'GPE']\n",
    "                gpe_phrases = [entity for entity in gpe_phrases if entity[0].isalpha() and entity[0].isupper()]\n",
    "                \n",
    "                gpe_list.extend([gpe for gpe in gpe_phrases if gpe not in noun_set])\n",
    "\n",
    "            total_gpe = FreqDist(gpe_list).most_common(10)\n",
    "            self.dictionary['GPE'].append(total_gpe)\n",
    "            return total_gpe\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Error: {e}')\n",
    "\n",
    "    def proper_name_searcher(self):\n",
    "        try:\n",
    "            phrase_list = self.phrase_searcher()\n",
    "            proper_list = []\n",
    "            for phrase in phrase_list:\n",
    "                tokens = nk.word_tokenize(phrase)\n",
    "                pos = nk.pos_tag(tokens)\n",
    "                tree = nk.ne_chunk(pos)\n",
    "                name_phrases = [' '.join(leaf[0] for leaf in node.leaves()) for node in tree if isinstance(node, nk.Tree) and node.label() == 'PERSON']\n",
    "                proper_list.extend(name_phrases)\n",
    "            \n",
    "            proper_name_list = FreqDist(proper_list).most_common(10)\n",
    "            self.dictionary['person'].append(proper_name_list)\n",
    "            return proper_name_list\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Error: {e}')\n",
    "        \n",
    "    def noun_ratio(self):\n",
    "        try:\n",
    "            phrase_list = self.phrase_searcher()\n",
    "            substantive_list = [(word, tag) for phrase in phrase_list for (word, tag) in nk.pos_tag(nk.word_tokenize(phrase)) if re.search(r'(NN|NNS|NNP|NNPS)', tag)]\n",
    "\n",
    "            total_substantive = FreqDist(substantive_list).most_common(10)\n",
    "            self.dictionary['nouns'].append(total_substantive)\n",
    "            return total_substantive\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Error: {e}')\n",
    "        \n",
    "    def verbs_ratio(self):\n",
    "        try:\n",
    "            phrase_list = self.phrase_searcher()\n",
    "            verbs_list = [(word, tag) for phrase in phrase_list for (word, tag) in nk.pos_tag(nk.word_tokenize(phrase)) if re.search(r'(VBD|VBG|VBN|VBP|VBZ)', tag)]\n",
    "\n",
    "            total_verbs = FreqDist(verbs_list).most_common(10)\n",
    "            self.dictionary['verbs'].append(total_verbs)\n",
    "            return total_verbs\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Error: {e}')    \n",
    "        \n",
    "\n",
    "    def data_searcher(self):\n",
    "        try:\n",
    "            phrase_list = self.phrase_searcher()\n",
    "            date_list = []\n",
    "            \n",
    "            pattern = r'\\b(?:\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}|january|february|march|april|may|june|july|august|september|october|november|december|monday|tuesday|wednesday|thursday|friday|saturday|sunday)\\b'\n",
    "\n",
    "            for phrase in phrase_list:\n",
    "                results = re.findall(pattern, phrase, re.IGNORECASE)\n",
    "                date_list.extend([date.lower() for date in results]) \n",
    "                \n",
    "            frequency_dist = FreqDist(date_list)\n",
    "            frequency_dist = [(date, freq) for date, freq in frequency_dist.items()]\n",
    "            frequency_dist = sorted(frequency_dist, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            self.dictionary['dates'].append(frequency_dist)\n",
    "            \n",
    "            return frequency_dist\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Error: {e}')\n",
    "\n",
    "    def markov_order_zero(self):\n",
    "        try:\n",
    "            total_tokens = self.token_collection()\n",
    "            freq_distribution = FreqDist(total_tokens)\n",
    "            phrase_list = self.phrase_searcher()  # Assumendo che questa lista sia già filtrata per i noun più frequenti\n",
    "            max_prob = 0\n",
    "            max_prob_phrase = None\n",
    "            final_sentence = []\n",
    "            for phrase in phrase_list:\n",
    "                tokens = nk.word_tokenize(phrase)\n",
    "                if 8 <= len(tokens) <= 12:\n",
    "                    prob = 1\n",
    "                    for token in tokens:\n",
    "                        ratio = freq_distribution[token] / len(total_tokens)\n",
    "                        prob *= ratio\n",
    "                    if prob > max_prob:\n",
    "                        max_prob = prob\n",
    "                        max_prob_phrase = phrase\n",
    "\n",
    "            final_sentence.append((max_prob_phrase, max_prob))\n",
    "            self.dictionary['markov_order'].append(final_sentence)\n",
    "    \n",
    "            return final_sentence\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Error: {e}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(first_par, second_par, output_file):\n",
    "    input_1 = cds.open(first_par, 'r', 'utf-8')\n",
    "    input_2 = cds.open(second_par, 'r', 'utf-8')\n",
    "    raw_1 = input_1.read()\n",
    "    raw_2 = input_2.read()\n",
    "    splitter = nk.data.load('tokenizers/punkt/english.pickle')\n",
    "    phrase_1 = splitter.tokenize(raw_1)\n",
    "    phrase_2 = splitter.tokenize(raw_2)\n",
    "\n",
    "    analyzer_1 = TextAnalyzer(phrase_1)\n",
    "    analyzer_2 = TextAnalyzer(phrase_2)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write('--'*40 + '\\n')\n",
    "        f.write(f'Text: {first_par}\\n')\n",
    "        f.write('--'*40 + '\\n')\n",
    "\n",
    "        # Most frequent proper nouns\n",
    "        analyzer_1.noun_search()\n",
    "        f.write('--Top 10 most frequent proper nouns in the text:\\n')\n",
    "        for noun, freq in analyzer_1.dictionary['NNP'][0]:\n",
    "            f.write(f'\\t{noun} with frequency {freq}\\n')\n",
    "\n",
    "        # Longest and shortest phrases for each proper noun\n",
    "        analyzer_1.phrase_length()\n",
    "        f.write('--For each proper noun, I provide the longest and shortest phrases:\\n')\n",
    "        for noun, phrases in analyzer_1.dictionary['phrases_length'][0].items():\n",
    "            f.write(f'\\tFor \\033[1m{noun}\\033[0m:\\n')\n",
    "            f.write(f'\\t{phrases[0]}\\n\\t{phrases[1]}\\n')\n",
    "\n",
    "        # Top 10 GPE\n",
    "        analyzer_1.geo_entity_search()\n",
    "        f.write('--For each sentence containing the most frequent nouns, I provide the 10 most frequent GPE:\\n')\n",
    "        for gpe in analyzer_1.dictionary['GPE'][0]:\n",
    "            f.write(f'\\t{gpe[0]} with frequency {gpe[1]}\\n')\n",
    "\n",
    "        # Top 10 proper names\n",
    "        analyzer_1.proper_name_searcher()\n",
    "        f.write('--For each sentence containing the most frequent nouns, I provide the 10 most frequent proper names:\\n')\n",
    "        for propn in analyzer_1.dictionary['person'][0]:\n",
    "            f.write(f'\\t{propn[0]} with frequency {propn[1]}\\n')\n",
    "\n",
    "        # Top 10 nouns\n",
    "        analyzer_1.noun_ratio()\n",
    "        f.write('--For each sentence containing the most frequent nouns, I provide the 10 most frequent nouns:\\n')\n",
    "        for substantive in analyzer_1.dictionary['nouns'][0]:\n",
    "            f.write(f'\\t{substantive[0][0]} with tag {substantive[0][1]} with frequency {substantive[1]}\\n') \n",
    "\n",
    "        # Top 10 verbs\n",
    "        analyzer_1.verbs_ratio()\n",
    "        f.write('--For each sentence containing the most frequent nouns, I provide the 10 most frequent verbs:\\n')\n",
    "        for verb in analyzer_1.dictionary['verbs'][0]:\n",
    "            f.write(f'\\t{verb[0][0]} with tag {verb[0][1]} with frequency {verb[1]}\\n') \n",
    "\n",
    "        # Dates, months, days\n",
    "        analyzer_1.data_searcher()\n",
    "        f.write('--For each sentence containing the most frequent nouns, I provide the dates, months, and days:\\n')\n",
    "        for data, frequency in analyzer_1.dictionary['dates'][0]:\n",
    "            f.write(f'\\t{data} with frequency {frequency}\\n')\n",
    "\n",
    "        # Markov model of order 0\n",
    "        analyzer_1.markov_order_zero()\n",
    "        f.write('--The phrase containing the most frequent nouns, at least 8 tokens long and at most 12 tokens long, with the highest probability is:\\n')\n",
    "        for phrase_markov, prob_markov in analyzer_1.dictionary['markov_order'][0]:\n",
    "            f.write(f'\\t{phrase_markov} with probability {prob_markov}\\n')\n",
    "\n",
    "        f.write('\\n\\n\\n')\n",
    "\n",
    "        # Analyze the second text\n",
    "        f.write('--'*40 + '\\n')\n",
    "        f.write(f'Text: {second_par}\\n')\n",
    "        f.write('--'*40 + '\\n')\n",
    "\n",
    "        # Most frequent proper nouns\n",
    "        analyzer_2.noun_search()\n",
    "        f.write('--Top 10 most frequent proper nouns in the text:\\n')\n",
    "        for noun, freq in analyzer_2.dictionary['NNP'][0]:\n",
    "            f.write(f'\\t{noun} with frequency {freq}\\n')\n",
    "\n",
    "        # Longest and shortest phrases for each proper noun\n",
    "        analyzer_2.phrase_length()\n",
    "        f.write('--For each proper noun, I provide the longest and shortest phrases:\\n')\n",
    "        for noun, phrases in analyzer_2.dictionary['phrases_length'][0].items():\n",
    "            f.write(f'\\tFor \\033[1m{noun}\\033[0m:\\n')\n",
    "            f.write(f'\\t{phrases[0]}\\n\\t{phrases[1]}\\n')\n",
    "\n",
    "        # Top 10 GPE\n",
    "        analyzer_2.geo_entity_search()\n",
    "        f.write('--For each sentence containing the most frequent nouns, I provide the 10 most frequent GPE:\\n')\n",
    "        for gpe in analyzer_2.dictionary['GPE'][0]:\n",
    "            f.write(f'\\t{gpe[0]} with frequency {gpe[1]}\\n')\n",
    "\n",
    "        # Top 10 proper names\n",
    "        analyzer_2.proper_name_searcher()\n",
    "        f.write('--For each sentence containing the most frequent nouns, I provide the 10 most frequent proper names:\\n')\n",
    "        for propn in analyzer_2.dictionary['person'][0]:\n",
    "            f.write(f'\\t{propn[0]} with frequency {propn[1]}\\n')\n",
    "\n",
    "        # Top 10 nouns\n",
    "        analyzer_2.noun_ratio()\n",
    "        f.write('--For each sentence containing the most frequent nouns, I provide the 10 most frequent nouns:\\n')\n",
    "        for substantive in analyzer_2.dictionary['nouns'][0]:\n",
    "            f.write(f'\\t{substantive[0][0]} with tag {substantive[0][1]} with frequency {substantive[1]}\\n') \n",
    "\n",
    "        # Top 10 verbs\n",
    "        analyzer_2.verbs_ratio()\n",
    "        f.write('--For each sentence containing the most frequent nouns, I provide the 10 most frequent verbs:\\n')\n",
    "        for verb in analyzer_2.dictionary['verbs'][0]:\n",
    "            f.write(f'\\t{verb[0][0]} with tag {verb[0][1]} with frequency {verb[1]}\\n') \n",
    "\n",
    "        # # Dates, months, days\n",
    "        analyzer_2.data_searcher()\n",
    "        f.write('--For each sentence containing the most frequent nouns, I provide the dates, months, and days:\\n')\n",
    "        for data, frequency in analyzer_2.dictionary['dates'][0]:\n",
    "            f.write(f'\\t{data} with frequency {frequency}\\n')\n",
    "\n",
    "        # Markov model of order 0\n",
    "        analyzer_2.markov_order_zero()\n",
    "        f.write('--The phrase containing the most frequent nouns, at least 8 tokens long and at most 12 tokens long, with the highest probability is:\\n')\n",
    "        for phrase_markov, prob_markov in analyzer_2.dictionary['markov_order'][0]:\n",
    "            f.write(f'\\t{phrase_markov} with probability {prob_markov}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    first_path = '../text/dracula-UTF8.txt'\n",
    "    second_path = '../text/hydeandjack-UTF8.txt'\n",
    "    output_file = '../output/output_program_2.out'\n",
    "    main(first_path, second_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
